{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94mrZebYa9hY"
      },
      "source": [
        "**tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z558lt3iS61x"
      },
      "outputs": [],
      "source": [
        "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_twUcotVsYTr"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "from typing import List , Dict\n",
        "from collections import Counter\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCrjl64MhRk1"
      },
      "outputs": [],
      "source": [
        "class BPE():\n",
        "\n",
        "  def __init__(self , input_path:str , vocab_size:int , special_tokens:List[str]):\n",
        "\n",
        "    with open(input_path) as f :\n",
        "      self.txt = f.read()\n",
        "      splitter = re.compile(\"|\".join(map(re.escape, special_tokens)))\n",
        "      self.txt = splitter.split(self.txt)\n",
        "    self.vocab_size = 0\n",
        "    #for t in self.txt:\n",
        "      #print(t)\n",
        "      #print('-----------------------------------------')\n",
        "    self.max_vocab_size  = vocab_size\n",
        "    self.special_tokens = special_tokens\n",
        "    self.vocab = {}\n",
        "    self.PAT  = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
        "\n",
        "    self.corpus = {}\n",
        "    self.corpus =  Counter([])\n",
        "    self.merges = []\n",
        "\n",
        "\n",
        "  def init_vocb(self):\n",
        "    for i in range(256):\n",
        "      self.vocab[i] = i.to_bytes()\n",
        "    self.vocab_size = len(self.vocab)\n",
        "    for s_t in self.special_tokens :\n",
        "      self.vocab[self.vocab_size] = s_t.encode(\"utf-8\")\n",
        "      self.vocab_size += 1\n",
        "\n",
        "\n",
        "  def pre_tokentize(self):\n",
        "\n",
        "    for t in self.txt:\n",
        "\n",
        "      matches = re.finditer(self.PAT , t)\n",
        "\n",
        "\n",
        "      for match in matches:\n",
        "        self.corpus.update([tuple(sb.to_bytes() for sb in list( match.group().encode(\"utf-8\")))])\n",
        "\n",
        "\n",
        "  def merge_iteration(self):\n",
        "    cnt = {}\n",
        "    merges_couple = tuple()\n",
        "    to_pick_couple = \"empty\"\n",
        "    to_pick = {to_pick_couple : 0}\n",
        "    tmp = tuple()\n",
        "    for c in self.corpus:\n",
        "      for i in range(len(c)-1):\n",
        "        couple = c[i] + c[i+1]\n",
        "\n",
        "        if couple in cnt:\n",
        "          cnt[couple][0] += self.corpus[c]\n",
        "          cnt[couple][1].append([c,i])\n",
        "        else :\n",
        "          cnt[couple] = [self.corpus[c],[[c,i]]]\n",
        "        if cnt[couple][0] > to_pick[to_pick_couple]:\n",
        "          tmp = ( c[i] , c[i+1])\n",
        "          to_pick_couple = couple\n",
        "          to_pick = {to_pick_couple : cnt[to_pick_couple][0]}\n",
        "          merges_couple = ( c[i] , c[i+1])\n",
        "\n",
        "        elif cnt[couple][0] == to_pick[to_pick_couple]:\n",
        "          tmp = ( c[i] , c[i+1])\n",
        "          to_pick_couple = max(tmp ,( c[i] , c[i+1]))\n",
        "          to_pick_couple = to_pick_couple[0] + to_pick_couple[1]\n",
        "          to_pick = {to_pick_couple : cnt[to_pick_couple][0]}\n",
        "          merges_couple = ( c[i] , c[i+1])\n",
        "\n",
        "\n",
        "    new_token = [len(self.vocab) ,to_pick_couple]\n",
        "    #print(to_pick_couple)\n",
        "    for sub_word in cnt[to_pick_couple][1]:\n",
        "      idx = sub_word[1]\n",
        "      new_sub_word = list(copy.deepcopy(sub_word[0]))\n",
        "      ln = len(new_sub_word)-2\n",
        "      bt = 0\n",
        "      while bt <= ln:\n",
        "\n",
        "        if new_sub_word[bt]+new_sub_word[bt+1] == to_pick_couple:\n",
        "          del new_sub_word[bt]\n",
        "          del new_sub_word[bt]\n",
        "          new_sub_word.insert(bt , to_pick_couple)\n",
        "          self.corpus[tuple(new_sub_word)] =self.corpus[sub_word[0]]\n",
        "          del self.corpus[sub_word[0]]\n",
        "          bt -= 1\n",
        "        if bt >= len(new_sub_word)-1:\n",
        "          break\n",
        "        bt+=1\n",
        "        ln = len(new_sub_word)-2\n",
        "    self.merges.append(merges_couple)\n",
        "    self.vocab[len(self.vocab)] = to_pick_couple\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "    pass\n",
        "\n",
        "  def test(self):\n",
        "    self.init_vocb()\n",
        "    self.pre_tokentize()\n",
        "    ln = len(self.vocab)\n",
        "    mx_ln = max([len(x) for x in self.corpus])\n",
        "    while  ln< self.max_vocab_size and mx_ln >1 :\n",
        "      self.merge_iteration()\n",
        "      ln = len(self.vocab)\n",
        "      mx_ln = max([len(x) for x in self.corpus])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YBzk9EibQ8u"
      },
      "source": [
        "language model transformer\n",
        "**bold text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWYarM84bduX"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "from einops import einsum ,rearrange\n",
        "\n",
        "class Linear(nn.Module):\n",
        "  def __init__(self,in_features, out_features, device=None, dtype=None):\n",
        "    super().__init__()\n",
        "    self.sigma = math.sqrt(2/(in_features+out_features))\n",
        "    self.w = nn.parameter.Parameter(nn.init.trunc_normal_(torch.zeros([out_features ,in_features]),0,self.sigma**2 , -3*self.sigma , 3* self.sigma))\n",
        "\n",
        "  def forward(self , x):\n",
        "\n",
        "    return einsum(self.w , x , \"i j , ... j -> ... i \")\n",
        "\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "  def __init__(self, num_embeddings, embedding_dim, device=None, dtype=None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.sigma = math.sqrt(2/(num_embeddings+embedding_dim))\n",
        "    self.E = nn.Parameter(nn.init.trunc_normal_(torch.zeros(num_embeddings , embedding_dim) , 0 ,self.sigma**2 , -3*self.sigma , 3*self.sigma ))\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.E[x]\n",
        "\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "  def __init__(self, d_model: int, eps: float = 1e-5, device=None, dtype=None):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.eps = eps\n",
        "    self.sigma = math.sqrt(2/d_model)\n",
        "    g = torch.zeros(d_model)\n",
        "    nn.init.trunc_normal_(g, 0, self.sigma**2, -3*self.sigma, 3*self.sigma)\n",
        "    self.g = nn.Parameter(g)\n",
        "\n",
        "  def forward(self,x):\n",
        "    g = self.g.to(x.device)\n",
        "    x_dtype = x.dtype\n",
        "    x = x.to(torch.float32)\n",
        "\n",
        "    rms = einsum(x,x ,\"... i , ... i -> ... i\")\n",
        "    rms = torch.sqrt(rms.sum(axis = -1 , keepdim = True) / self.d_model +self.eps).to(device)\n",
        "\n",
        "    x = x /rms* g\n",
        "    return x.to(x_dtype)\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "  def __init__(self, d_model ,  dff = None ,device=None, dtype=None) :\n",
        "    super().__init__()\n",
        "    self.dff = dff\n",
        "    if dff is None:\n",
        "      self.dff = int(8/3 * d_model)\n",
        "    self.sigma = math.sqrt(2/(d_model+self.dff))\n",
        "    self.w1 = nn.parameter.Parameter(nn.init.trunc_normal_(torch.zeros(self.dff , d_model) , 0,self.sigma**2 , -3*self.sigma,3*self.sigma))\n",
        "    self.w2 = nn.parameter.Parameter(nn.init.trunc_normal_(torch.zeros( d_model,self.dff ) , 0,self.sigma**2 , -3*self.sigma,3*self.sigma))\n",
        "    self.w3 = nn.parameter.Parameter(nn.init.trunc_normal_(torch.zeros(self.dff , d_model) , 0,self.sigma**2 , -3*self.sigma,3*self.sigma))\n",
        "\n",
        "  def forward(self,x):\n",
        "    x_ = einsum(self.w1 ,x , \" ... i k, ... k -> ... i\")\n",
        "    x_ = torch.sigmoid(x_) * x_\n",
        "    x_ = einsum(self.w3 ,x , \" ... i k, ... k -> ... i\") * x_\n",
        "    return einsum(self.w2 ,  x_  , \"... i k , ... k -> ... i\")\n",
        "\n",
        "class RotaryPositionalEmbedding(nn.Module):\n",
        "  def __init__(self, theta: float, d_k: int, max_seq_len: int, device=None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.theta = theta\n",
        "    self.d_k = d_k\n",
        "    self.max_seq_len = max_seq_len\n",
        "\n",
        "    self.token_indecies = torch.arange(0,max_seq_len ,1).view(max_seq_len,1)\n",
        "    self.embedding_indecies = torch.arange(0,d_k/2 , 1).view(1,int(d_k/2))\n",
        "\n",
        "    cos = torch.cos(self.token_indecies/(self.theta**((2*self.embedding_indecies)/self.d_k)))\n",
        "    sin = torch.sin(self.token_indecies/(self.theta**((2*self.embedding_indecies)/self.d_k)))\n",
        "    self.register_buffer(name = \"cos\" , tensor=cos , persistent=False)\n",
        "    self.register_buffer(name = \"sin\" , tensor=sin , persistent=False)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self , x , token_positions):\n",
        "    cos_s = self.cos[token_positions]\n",
        "    sin_s = self.sin[token_positions]\n",
        "    rotation_matrix = torch.stack((cos_s , -sin_s , sin_s ,cos_s) ,dim = -1)\n",
        "    rotation_matrix = rearrange(rotation_matrix , \"... (d1 d2) -> ... d1 d2\" , d1 = 2)\n",
        "\n",
        "    x = rearrange(x , \"... (d1 d2) -> ... d1 d2 \" , d2 = 2)\n",
        "\n",
        "    x = einsum(rotation_matrix , x , \" ... s k i j , ... s k j -> ... s k i \")\n",
        "    x = rearrange(x , \"... d1 d2 -> ... (d1 d2)\")\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Softmax(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self , x,i = -1):\n",
        "    c = torch.max(x, dim = i , keepdim=True)\n",
        "    x = torch.exp(x-c.values)\n",
        "    return x / torch.sum((x) , dim=i ,  keepdim=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ScaleDotProductSelfAtention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.softmax = Softmax()\n",
        "\n",
        "  def forward(self , k , q , v , mask = None):\n",
        "    if mask is None :\n",
        "      mask = torch.ones(q.shape[-2] , k.shape[-2]).to(torch.bool)\n",
        "\n",
        "    mask = torch.logical_not(mask)\n",
        "    mask = mask.to(torch.float).masked_fill(mask,-float(\"inf\"))\n",
        "    mask = mask.to(k.device)\n",
        "\n",
        "    product = einsum(q,k , \"... i k , ... j k -> ... i j\")/math.sqrt(k.shape[-1])\n",
        "    #product = torch.matmul(q,torch.transpose(k , -1 ,-2))/k.shape[-1]\n",
        "    print(product.device)\n",
        "    print(mask.device)\n",
        "\n",
        "    product += mask\n",
        "\n",
        "    product = self.softmax(product , -1)\n",
        "\n",
        "    product = einsum(product , v ,\"... i k , ... k j -> ... i j\")\n",
        "    #product = torch.matmul(product , v)\n",
        "    return product\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "  def __init__(self , d_model , n_heads , theta  = 10000.0, max_seq_len = 1024 ):\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.projection_size = d_model\n",
        "    self.d_h = int(d_model / n_heads)\n",
        "    self.sigma = math.sqrt(2/(d_model + self.projection_size))\n",
        "    self.d_heads = d_model / n_heads\n",
        "    self.wq = Linear(d_model , d_model)\n",
        "    self.wk = Linear(d_model , d_model)\n",
        "    self.wv = Linear(d_model , d_model)\n",
        "    self.wo = Linear(d_model , d_model )\n",
        "    self.rope = RotaryPositionalEmbedding(theta , self.d_h , max_seq_len)\n",
        "    self.attention = ScaleDotProductSelfAtention()\n",
        "\n",
        "  def forward(self,x,token_positions = None):\n",
        "    k = torch.stack(self.wk(x).split(self.d_h , -1) , dim =-3)\n",
        "    q = torch.stack(self.wq(x).split(self.d_h , -1) , dim =-3)\n",
        "    v = torch.stack(self.wv(x).split(self.d_h , -1) , dim =-3)\n",
        "\n",
        "    if token_positions == None :\n",
        "      token_positions = torch.arange(0  ,k.shape[-2])\n",
        "\n",
        "\n",
        "\n",
        "    k , q = self.rope(k,token_positions) , self.rope(q,token_positions)\n",
        "\n",
        "    mask = torch.tril(torch.ones((k.shape[-2],k.shape[-2]))).to(torch.bool)\n",
        "\n",
        "    calculated_attention =  self.attention(k,q,v,mask)\n",
        "    calculated_attention  = rearrange(calculated_attention , \"... head seq_len feature -> ... seq_len(head feature) \")\n",
        "\n",
        "    return self.wo(calculated_attention)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self , d_model , num_heads , d_ff  , theta = 10000.0 , max_seq_len = 1024 ,device = \"cpu\"):\n",
        "    super().__init__()\n",
        "    self.norm1 = RMSNorm(d_model)\n",
        "    self.norm2 = RMSNorm(d_model)\n",
        "    self.attention = MultiHeadSelfAttention(d_model , num_heads ,theta ,max_seq_len)\n",
        "    self.feed_forward = PositionWiseFeedForward(d_model , d_ff , max_seq_len )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self , x , token_positions = None):\n",
        "\n",
        "\n",
        "    result = self.norm1(x)\n",
        "    result = self.attention(result , token_positions)\n",
        "    x = x + result\n",
        "\n",
        "    result = self.norm2(x)\n",
        "    result = self.feed_forward(result)\n",
        "    result = x + result\n",
        "    return result\n",
        "\n",
        "\n",
        "class MultiInputSequential(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        for module in self:\n",
        "            if isinstance(inputs, tuple):\n",
        "                inputs = module(*inputs)\n",
        "            else:\n",
        "                inputs = module(inputs)\n",
        "        return inputs\n",
        "\n",
        "\n",
        "class TransformerLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self,vocab_size , max_seq_len ,num_layers,d_model , num_heads ,d_ff , theta=10000.0 ):\n",
        "    super().__init__()\n",
        "    self.embedding = Embedding(vocab_size,d_model)\n",
        "    self.layers = MultiInputSequential(*[TransformerBlock(d_model , num_heads, d_ff ,theta , max_seq_len) for _ in range(num_layers)])\n",
        "    self.norm = RMSNorm(d_model)\n",
        "    self.linear = Linear(d_model , vocab_size)\n",
        "    self.classifier = Softmax()\n",
        "\n",
        "    for i in range(len(self.layers)-1):\n",
        "      print(self.layers[i] == self.layers[i])\n",
        "\n",
        "  def forward(self , x, token_positions = None):\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    x = self.layers(x , token_positions)\n",
        "    x = self.norm(x)\n",
        "    x = self.linear(x)\n",
        "    #x = self.classifier(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class CrossEntropyLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self,x , labels):\n",
        "    x = x- torch.max(x , dim = -1 , keepdim=True).values\n",
        "    labels = labels.view(*x.shape[:-1],1)\n",
        "    log_of_sum = torch.log(torch.sum(torch.exp(x) , dim = -1 ,keepdim=True))\n",
        "    x =  torch.gather( x ,  -1 , labels)\n",
        "    return torch.sum(log_of_sum - x) / torch.prod(torch.tensor(labels.shape))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vw06yZS7fT4m"
      },
      "outputs": [],
      "source": [
        "from collections.abc import Callable, Iterable\n",
        "from typing import Optional\n",
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "from einops import einsum ,rearrange\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Loader(Dataset):\n",
        "    def __init__(self, data , batch_size , context_length , device = None):\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "        self.context_length = context_length\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.context_length\n",
        "\n",
        "    def __getitem__(self):\n",
        "        starts = np.random.randint(0 , self.__len__()-self.context_length-1,(self.batch_size , 1))\n",
        "        increments = np.expand_dims(np.arange(self.context_length) , axis=0)\n",
        "        y = starts+1 + increments\n",
        "        x = starts + increments\n",
        "\n",
        "        x = self.data[x]\n",
        "        y = self.data[y]\n",
        "        return (torch.from_numpy(x).to(self.device),torch.from_numpy(y).to(self.device))\n",
        "\n",
        "class AdamW(torch.optim.Optimizer):\n",
        "  def __init__(self, params , lr=1e-3 ,betas = (0.9 , 0.999) ,eps = 1e-8 , weight_decay = 1e-4 , a_min  = 1e-4, a_max = 1e-2 , Tw = 100 ,Tc = 3000):\n",
        "    if lr < 0:\n",
        "      raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "    self.defaults = {\"lr\": lr , \"b1\" :betas[0] , \"b2\" :betas[1]  , \"eps\" : eps , \"lambda\" : weight_decay , \"a_min\" :a_min , \"a_max\" : a_max , \"tw\":Tw , \"tc\" :Tc}\n",
        "    super().__init__(params, self.defaults)\n",
        "\n",
        "    for group in self.param_groups:\n",
        "\n",
        "      for p in group[\"params\"]:\n",
        "        self.state[p] = {\"m\" :torch.zeros_like(p.data) , \"v\" : torch.zeros_like(p.data)}\n",
        "\n",
        "  def step(self, closure: Optional[Callable] = None ):\n",
        "    loss = None if closure is None else closure()\n",
        "\n",
        "    for group in self.param_groups:\n",
        "      gardient_clipping(group[\"params\"] , 1.0)\n",
        "      for p in group[\"params\"]:\n",
        "\n",
        "        if p.grad is None:\n",
        "          continue\n",
        "\n",
        "        state = self.state[p] # Get state associated with p.\n",
        "        t = state.get(\"t\", 1) # Get iteration number from the state, or initial value.\n",
        "        lr = cosine_annealing_schhduler(group[\"a_min\"] , group[\"a_max\"], t , group[\"tw\"] ,group[\"tc\"])\n",
        "        group[\"lr\"] = lr\n",
        "        grad = p.grad.data # Get the gradient of loss with respect to p.\n",
        "        p.grad\n",
        "        m = state[\"m\"] * group[\"b1\"] + (1-group[\"b1\"]) * grad\n",
        "        v = state[\"v\"] * group[\"b2\"] + (1-group[\"b2\"]) * grad**2\n",
        "        self.state[p][\"m\"] , self.state[p][\"v\"] = m,v\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        alpha_t = lr * math.sqrt((1-group[\"b2\"]**t))/(1-group[\"b1\"]**t)\n",
        "        data = p.data\n",
        "        p.data -= alpha_t * m /(torch.sqrt(v) + group[\"eps\"])\n",
        "\n",
        "        p.data -= lr * group[\"lambda\"] * p.data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        state[\"t\"] = t + 1 # Increment iteration number.\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "def cosine_annealing_schhduler(a_min , a_max, t , Tw ,Tc):\n",
        "\n",
        "  if t < Tw:\n",
        "    return t /Tw * a_max\n",
        "\n",
        "  if Tw <= t and t<= Tc :\n",
        "    return a_min + 1/2*(1 + math.cos((t-Tw)/(Tc-Tw)*math.pi)) *(a_max - a_min)\n",
        "\n",
        "  return a_min\n",
        "\n",
        "\n",
        "def gardient_clipping(parameters , max_l2_norm ,eps = 10e-6):\n",
        "\n",
        "  grads = [p.grad for p in parameters if p.grad is not None]\n",
        "\n",
        "  if len(grads) == 0:\n",
        "        return\n",
        "\n",
        "  l2_norm = torch.sqrt(sum(torch.sum(g ** 2) for g in grads))\n",
        "\n",
        "\n",
        "  for p in parameters:\n",
        "    grad = p.grad\n",
        "    if grad is None :\n",
        "      continue\n",
        "\n",
        "    if  l2_norm > max_l2_norm:\n",
        "\n",
        "      p.grad = p.grad  *   max_l2_norm/(l2_norm+eps)\n",
        "\n",
        "\n",
        "\n",
        "def save_checkpoint(model,optimizer, iteration , loss , out):\n",
        "\n",
        "    torch.save({\"model\" : model.state_dict(),\"optimizer\" : optimizer.state_dict() , \"iteration\" : iteration , \"loss\" : loss},out)\n",
        "\n",
        "def load_checkpoint(src, model, optimizer):\n",
        "  ckpt = torch.load(src)\n",
        "\n",
        "  model.load_state_dict(ckpt[\"model\"])\n",
        "  optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "  return ckpt[\"iteration\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(model  ,prompt , softmax , temperature ,nucleus , token_number , max_token_number , eos_token = 19, response = []):\n",
        "\n",
        "  if token_number > max_token_number :\n",
        "    return response\n",
        "\n",
        "  if response :\n",
        "    if response[-1] == eos_token:\n",
        "      return response\n",
        "\n",
        "  out = model(prompt)[-1]/temperature\n",
        "  out = softmax(out)\n",
        "\n",
        "  out  , indecies = torch.sort(out , descending = True)\n",
        "\n",
        "  top_p , top_p_indecies = [] , []\n",
        "\n",
        "  sum = 0\n",
        "  i = 0\n",
        "  while sum < nucleus and i<(len(out)):\n",
        "    sum += out[i]\n",
        "    top_p.append(out[i].item())\n",
        "    top_p_indecies.append(indecies[i].item())\n",
        "    i+=1\n",
        "\n",
        "  top_p = torch.tensor(top_p)\n",
        "  top_p = top_p/torch.sum(top_p)\n",
        "\n",
        "  pick = torch.multinomial(top_p , 1 )\n",
        "  next_token = top_p_indecies[pick]\n",
        "  response.append(next_token)\n",
        "  prompt.append(next_token)\n",
        "\n",
        "  return decode(model  ,prompt , softmax , temperature ,nucleus , token_number+1 , max_token_number , eos_token,response)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zztKgQx7rX6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P6OklKvuQaw"
      },
      "source": [
        "**training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orcE331_uPxv",
        "outputId": "16819439-f387-48ad-d6e5-2ad86d6559e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "#from components import *\n",
        "import numpy as np\n",
        "import os\n",
        "import wandb\n",
        "key=\"put_key_here\"\n",
        "wandb.login(key=key)\n",
        "\n",
        "def train_language_model(project ,checkpointig_path,checkpoint_every_n_iterations,data ,number_of_iterations, vocab_size, max_seq_len, num_layers, d_model, num_heads, d_ff,  batch_size = 128,theta = 10000, lr= 0.001, betas = (0.9, 0.999), eps = 1e-8, weight_decay= 0.0001,src = None   ,device = \"cpu\"):\n",
        "  model = TransformerLanguageModel(vocab_size, max_seq_len,num_layers,d_model,num_heads, d_ff, theta)\n",
        "  model.to(device)\n",
        "\n",
        "\n",
        "  optimizer = AdamW(model.parameters(),lr,betas,eps,weight_decay , Tw = 30 , Tc = 100)\n",
        "  CE = CrossEntropyLoss()\n",
        "  if src:\n",
        "    ckpt = torch.load(src)\n",
        "\n",
        "    model.load_state_dict(ckpt[\"model\"])\n",
        "    optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "\n",
        "\n",
        "  if not isinstance(data , np.ndarray):\n",
        "    data = np.memmap(data , dtype=np.int32)\n",
        "\n",
        "  loader = Loader(data , batch_size,max_seq_len , device)\n",
        "\n",
        "  config = {\n",
        "      \"learning rate\" : lr,\n",
        "      \"betas\" : betas,\n",
        "      \"weight decay\" : weight_decay ,\n",
        "      \"iterations\" : number_of_iterations\n",
        "\n",
        "  }\n",
        "\n",
        "\n",
        "  with wandb.init(project=project, config=config) as run:\n",
        "\n",
        "    for i in range(number_of_iterations):\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      x , y = loader.__getitem__()\n",
        "      _y = model(x)\n",
        "      loss = CE(_y,y)\n",
        "      loss.backward()\n",
        "      print(\"loss : \" ,str(loss))\n",
        "      if (i) % checkpoint_every_n_iterations == 0 :\n",
        "        save_checkpoint(model, optimizer , i,loss , os.path.join(checkpointig_path , \"ckpt_n_\"+str(i)+\".pt\"))\n",
        "\n",
        "      optimizer.step()\n",
        "      run.log({\"loss\" : float(loss) ,\"abdou\" : 5, \"lr\": optimizer.param_groups[0][\"lr\"] , \"b1\" :optimizer.param_groups[0][\"b1\"] , \"b2\" :optimizer.param_groups[0][\"b2\"]  , \"eps\" : optimizer.param_groups[0][\"eps\"] , \"lambda\" : optimizer.param_groups[0][\"lambda\"]})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bGxrY3N4XB3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lpa8td61NH8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n"
      ],
      "metadata": {
        "id": "LBp_VjJudAG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_iterator():\n",
        "    for ex in ds:\n",
        "        yield ex[\"text\"]\n"
      ],
      "metadata": {
        "id": "vsyOMpw5dTnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
        "#tokenizer = Tokenizer.from_file(\"/content/tinystories_bpe.json\")\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Trainer\n",
        "trainer = BpeTrainer(\n",
        "    vocab_size=1000,\n",
        "    special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
        ")\n",
        "\n",
        "# Train\n",
        "tokenizer.train_from_iterator(text_iterator(), trainer)\n"
      ],
      "metadata": {
        "id": "9avzwlKod8aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save(\"tinystories_bpe.json\")\n",
        "bos = tokenizer.token_to_id(\"<bos>\")\n",
        "eos = tokenizer.token_to_id(\"<eos>\")"
      ],
      "metadata": {
        "id": "Fu9l-vYTmPoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "total_tokens = 0\n",
        "for ex in ds:\n",
        "    total_tokens += len(tokenizer.encode(ex[\"text\"]).ids) + 2\n",
        "\n",
        "print(\"Total tokens:\", total_tokens)\n"
      ],
      "metadata": {
        "id": "gZpVZR6fgDeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "mmap_path = \"tinystories_tokens.memmap\"\n",
        "\n",
        "tokens = np.memmap(\n",
        "    mmap_path,\n",
        "    dtype=np.int32,\n",
        "    mode=\"w+\",\n",
        "    shape=(571984873,)\n",
        ")\n",
        "idx = 0\n",
        "\n",
        "for ex in ds:\n",
        "    tokens[idx] = bos\n",
        "    idx += 1\n",
        "\n",
        "    ids = tokenizer.encode(ex[\"text\"]).ids\n",
        "    tokens[idx:idx+len(ids)] = ids\n",
        "    idx += len(ids)\n",
        "\n",
        "    tokens[idx] = eos\n",
        "    idx += 1\n",
        "\n",
        "tokens.flush()\n"
      ],
      "metadata": {
        "id": "pfjTTuMvlGm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir checkpoints"
      ],
      "metadata": {
        "id": "2CuiJAu6rhoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_iterations = 1000\n",
        "vocab_size = 10000\n",
        "max_seq_len = 256\n",
        "num_layers = 4\n",
        "d_model = 512\n",
        "num_heads = 16\n",
        "d_ff = 1344\n",
        "batch_size = 64\n",
        "theta = 10000\n",
        "lr= 0.001\n",
        "betas = (0.9, 0.999)\n",
        "eps = 1e-8\n",
        "weight_decay= 0.0001\n",
        "src = None   ,\n",
        "device = torch.cuda.current_device()\n",
        "data = \"/content/tinystories_tokens.memmap\"\n",
        "\n",
        "\n",
        "train_language_model(\"test\",\"checkpoints\",10,data , number_of_iterations , vocab_size , max_seq_len , num_layers , d_model , num_heads,d_ff,batch_size , device = device)"
      ],
      "metadata": {
        "id": "L4-8TMZOrf_U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}