# Transformer from Scratch

PyTorch implementation of a decoder-only transformer, built for learning.  
**Trained on:** TinyStories (570M tokens)  
**Architecture:** 4 layers, d_model=512, 16 heads, RoPE, RMSNorm, SwiGLU FFN  
**Features:** Custom AdamW with cosine annealing, gradient clipping, checkpointing  

**Note:** This is a personal implementation for educational depth. Code is functional but not optimized for production.
